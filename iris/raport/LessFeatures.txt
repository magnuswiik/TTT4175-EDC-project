Bias var feil implementert
Verste null fjernet 30 last:
Best Alpha and ErrorMargin, with ErrorRates was:  [0.0382 0.     5.56  ]
Verste null fjernet:
Best Alpha and ErrorMargin, with ErrorRates was:  [0.0065 3.33   3.33  ]
Verst en fjernet:
Best Alpha and ErrorMargin, with ErrorRates was:  [3.70e-03 1.67e+00 4.44e+00]
Verste to fjernet:
Best Alpha and ErrorMargin, with ErrorRates was:  [0.0305 3.33   6.67  ]
Verste tre fjernet:
Best Alpha and ErrorMargin, with ErrorRates was:  [0.0136 6.67   4.44  ]






Removing the worst feature as seen in the histogram (sepal width) we got the following results:

At 10**3 itterations with alpha 0.0061:
ConfusionMatrixTestSet: 
 [[20.  0.  0.]
 [ 0. 18.  2.]
 [ 0.  2. 18.]]
ConfusionMatrixTrainingSet: 
 [[30.  0.  0.]
 [ 0. 25.  5.]
 [ 0.  1. 29.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [6.67%, 6.67%]

At 10**4 itterations:
ConfusionMatrixTestSet: 
 [[20.  0.  0.]
 [ 0. 18.  2.]
 [ 0.  3. 17.]]
ConfusionMatrixTrainingSet: 
 [[30.  0.  0.]
 [ 0. 26.  4.]
 [ 0.  1. 29.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [8.33%, 5.56%]

At 10**5 itterations:
ConfusionMatrixTestSet: 
 [[20.  0.  0.]
 [ 0. 18.  2.]
 [ 0.  2. 18.]]
ConfusionMatrixTrainingSet: 
 [[30.  0.  0.]
 [ 0. 27.  3.]
 [ 0.  1. 29.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [6.67%, 4.44%]

Clearly the algorithm gets worse when removing a sepperation feature. Would be interesting to find an optimal alpha


Finding a good alpha for one removed feature gave alpha = 1.9*10**(-3)

At 10**3 itterations with alpha 1.9*10**(-3):
ConfusionMatrixTestSet: 
 [[20.  0.  0.]
 [ 0. 18.  2.]
 [ 0.  0. 20.]]
ConfusionMatrixTrainingSet: 
 [[30.  0.  0.]
 [ 0. 23.  7.]
 [ 0.  0. 30.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [3.33%, 7.78%]

At 10**4 itterations with alpha 1.9*10**(-3):
ConfusionMatrixTestSet: 
 [[20.  0.  0.]
 [ 0. 18.  2.]
 [ 0.  3. 17.]]
ConfusionMatrixTrainingSet: 
 [[30.  0.  0.]
 [ 0. 25.  5.]
 [ 0.  1. 29.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [8.33%, 6.67%]

At 10**5 itterations with alpha 1.9*10**(-3):
ConfusionMatrixTestSet: 
 [[20.  0.  0.]
 [ 0. 18.  2.]
 [ 0.  2. 18.]]
ConfusionMatrixTrainingSet: 
 [[30.  0.  0.]
 [ 0. 26.  4.]
 [ 0.  1. 29.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [6.67%, 5.56%]




Removing the two worst features as seen in the histogram (sepal width, sepal length) we got the following results:

At 10**3 itterations with alpha 0.0061:
ConfusionMatrixTestSet: 
 [[18.  1.  1.]
 [ 0. 12.  8.]
 [ 0.  3. 17.]]
ConfusionMatrixTrainingSet: 
 [[25.  5.  0.]
 [ 1. 14. 15.]
 [ 0.  6. 24.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [21.67%, 30%]

At 10**4 itterations:
ConfusionMatrixTestSet: 
 [[18.  1.  1.]
 [ 0. 17.  3.]
 [ 1.  3. 16.]]
ConfusionMatrixTrainingSet: 
 [[26.  4.  0.]
 [ 3. 16. 11.]
 [ 0.  8. 22.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [15%, 28.89%]

At 10**5 itterations:
ConfusionMatrixTestSet: 
 [[18.  1.  1.]
 [ 0. 17.  3.]
 [ 1.  3. 16.]]
ConfusionMatrixTrainingSet: 
 [[26.  4.  0.]
 [ 3. 17. 10.]
 [ 0.  8. 22.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [15%, 27.78%]



Finding a good alpha for two removed features gave alpha = 2.04*10**(-2)

At 10**3 itterations with alpha 2.04*10**(-2):
ConfusionMatrixTestSet: 
 [[18.  1.  1.]
 [ 0. 17.  3.]
 [ 1.  3. 16.]]
ConfusionMatrixTrainingSet: 
 [[26.  4.  0.]
 [ 2. 17. 11.]
 [ 0.  8. 22.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [15%, 27.78%]

At 10**4 itterations with alpha 2.04*10**(-2):
ConfusionMatrixTestSet: 
 [[18.  1.  1.]
 [ 0. 17.  3.]
 [ 1.  4. 15.]]
ConfusionMatrixTrainingSet: 
 [[26.  4.  0.]
 [ 2. 18. 10.]
 [ 0. 10. 20.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [16.67%, 28.89%]

At 10**5 itterations with alpha 2.04*10**(-2):
ConfusionMatrixTestSet: 
 [[18.  1.  1.]
 [ 0. 17.  3.]
 [ 1.  4. 15.]]
ConfusionMatrixTrainingSet: 
 [[26.  4.  0.]
 [ 2. 18. 10.]
 [ 0. 10. 20.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [16.67%, 28.89%]




Removing the three worst features as seen in the histogram (sepal width, sepal length, petal length) we got the following results:

At 10**3 itterations with alpha 0.0061:
ConfusionMatrixTestSet: 
 [[ 0.  0. 20.]
 [ 0.  0. 20.]
 [ 0.  0. 20.]]
ConfusionMatrixTrainingSet: 
 [[ 0.  0. 30.]
 [ 0.  0. 30.]
 [ 0.  0. 30.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [66.67%, 66.67%]

At 10**4 itterations:
ConfusionMatrixTestSet: 
 [[ 0.  0. 20.]
 [ 0.  0. 20.]
 [ 0.  0. 20.]]
ConfusionMatrixTrainingSet: 
 [[ 0.  0. 30.]
 [ 0.  0. 30.]
 [ 0.  0. 30.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [66.67%, 66.67%]

At 10**5 itterations:
ConfusionMatrixTestSet: 
 [[ 0.  0. 20.]
 [ 0.  0. 20.]
 [ 0.  0. 20.]]
ConfusionMatrixTrainingSet: 
 [[ 0.  0. 30.]
 [ 0.  0. 30.]
 [ 0.  0. 30.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [66.67%, 66.67%]



Finding a good alpha for three removed features gave alpha = 2*10**(-4)
At 10**3 itterations with alpha 2*10**(-4):
ConfusionMatrixTestSet: 
 [[ 0.  0. 20.]
 [ 0.  0. 20.]
 [ 0.  0. 20.]]
ConfusionMatrixTrainingSet: 
 [[ 0.  0. 30.]
 [ 0.  0. 30.]
 [ 0.  0. 30.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [66.67%, 66.67%]

At 10**4 itterations with alpha 2*10**(-4):
ConfusionMatrixTestSet: 
 [[ 0.  0. 20.]
 [ 0.  0. 20.]
 [ 0.  0. 20.]]
ConfusionMatrixTrainingSet: 
 [[ 0.  0. 30.]
 [ 0.  0. 30.]
 [ 0.  0. 30.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [66.67%, 66.67%]

At 10**5 itterations with alpha 2*10**(-4):
ConfusionMatrixTestSet: 
 [[ 0.  0. 20.]
 [ 0.  0. 20.]
 [ 0.  0. 20.]]
ConfusionMatrixTrainingSet: 
 [[ 0.  0. 30.]
 [ 0.  0. 30.]
 [ 0.  0. 30.]]
ErrorRate [testSetErrorRate,trainingSetErrorRate] = [66.67%, 66.67%]